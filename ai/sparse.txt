sparse 代表数据为0，sparse数据的存在让不为0的dense数据聚集在一起；因为存在数据聚集效应，所以才能学到特征和规律；如果数据维度很高，噪音很多，原本为0的位置，占比会越来越少，稀疏区域在消失；对应的dense数据的聚集效应减弱，因为看上去全是数据，看不见产生聚集效应的稀疏隔离区域；稀疏数据占比减少，导致数据聚集效应的消失，导致特征学习变得困难

-------------------------------- -------------------------------- -------------------------------- --------------------------------
Embedding为代表的表征学习，在大规模稀疏ID问题和多值ID问题上，似乎也能搞定自动化特征工程，于是互大家看到很多，各种千奇百怪的文本分类模型，CTR预估模型，似乎就是Embedding后面一顿盘他，最终转化成分类问题，似乎并不需要特征工程这门祖传的手艺了。

Kaggle上有一句非常经典的话，数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已，而这恰恰是课堂上最为缺失，一门需要在实践中学习的手艺。


